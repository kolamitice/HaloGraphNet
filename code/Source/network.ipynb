{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs are tuples of a set of nodes, which can be connected with edges if there is a relationship between them. This connections can be directed or undirected. Each node has a feature vector which can be concatenated into a feature matrix. This work will take use of simple (without self-loops) and undirected graphs. Each graph represents a different halo, where the nodes are the subhalos within the corresponding hosting halo. The chosen subhalo features are: 3D comoving position, stellar mass, magnitude of velocity and the stellar half mass radius (radius containing half of the stellar mass in the galaxy). There are several ways to make connect the nodes. Gravity is a long-range force every node should be connected to all others. Since the effect is greater at smaller distances we choose a radius as a hyperparameter in which a node is connected to all others within the radius. The optimal radius is so large that 98% of all graphs are complete (all nodes are connected to each other):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNNs make use of an mechanism called message passing. This means that each node aggregates information by all nodes in its neighbourhood in order to update its feature vector into the so called new \"hidden feature vector\". The author discovered that for this particular purpose the design of an Edge Convolution as describe in (https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h_i = \\max_{j\\in N(i)} \\psi ([x_i, x_i-x_j])\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\psi$ denotes a multi layer perceptron with an input layer with $2*n_{feat}$ channels and 3 hidden layers with 300, 300 and 100 hidden channels seperated by ReLu activation function works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing \n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_cluster import radius_graph\n",
    "\n",
    "# Edge convolution layer\n",
    "class EdgeLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(EdgeLayer, self).__init__(aggr='max') #  \"Max\" aggregation.\n",
    "        self.mlp = Sequential(Linear(2 * in_channels, mid_channels),\n",
    "                       ReLU(),\n",
    "                       Linear(mid_channels, mid_channels),\n",
    "                       ReLU(),\n",
    "                       Linear(mid_channels, out_channels))\n",
    "        self.messages = 0.\n",
    "        self.input = 0.\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        # x_i has shape [E, in_channels]\n",
    "        # x_j has shape [E, in_channels]\n",
    "\n",
    "        input = torch.cat([x_i, x_j - x_i], dim=-1)  # tmp has shape [E, 2 * in_channels]\n",
    "\n",
    "        self.input = input\n",
    "        self.messages = self.mlp(input)\n",
    "\n",
    "        return self.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General GNN architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden feature vectors are put into a pooling layer, where the dimension is reduced from n_feat*n_nodes to 20 by concatinating the aggregated features via adding, max and mean with the global properties. This new global feature vector is then put into a MLP with 3 hidden layers with 300 channels seperated by ReLu functions and an output layer consisting of the target and its standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "y = \\phi(\\bigoplus_{i \\in G_h} h_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\psi$ denotes the MLP and $\\bigoplus$ some aggregation. In this implementation max, mean as well as sum have been used to create global features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGNN(torch.nn.Module):\n",
    "    def __init__(self, use_model, node_features, n_layers, k_nn, hidden_channels=300, latent_channels=100, loop=False):\n",
    "        super(ModelGNN, self).__init__()\n",
    "\n",
    "        in_channels = node_features\n",
    "\n",
    "        #Graph layer (hyperparameter optimization said only 1 is sufficient)\n",
    "        self.layer = EdgeLayer(in_channels, hidden_channels, latent_channels)\n",
    "\n",
    "        lin_in = latent_channels*3+2 # BIG ?????  should be 20???\n",
    "        self.lin = Sequential(Linear(lin_in, latent_channels),\n",
    "                              ReLU(),\n",
    "                              Linear(latent_channels, latent_channels),\n",
    "                              ReLU(),\n",
    "                              Linear(latent_channels, 2))\n",
    "\n",
    "        self.k_nn = k_nn #hyperparameter\n",
    "        self.pooled = 0.\n",
    "        self.h = 0.\n",
    "        self.loop = loop #no selfloops loop = False\n",
    "        self.namemodel = use_model\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, pos, batch, u = data.x, data.pos, data.batch, data.u\n",
    "\n",
    "        # Get edges using positions by computing the neighbors within a radius\n",
    "        edge_index = radius_graph(pos, r=self.k_nn, batch=batch, loop=self.loop)\n",
    "\n",
    "        # Start message passing\n",
    "        self.h = x\n",
    "        x = x.relu()\n",
    "\n",
    "\n",
    "        # Mix different global pooling layers\n",
    "        addpool = global_add_pool(x, batch) # [num_examples, hidden_channels]\n",
    "        meanpool = global_mean_pool(x, batch)\n",
    "        maxpool = global_max_pool(x, batch)\n",
    "        self.pooled = torch.cat([addpool, meanpool, maxpool, u], dim=1) #dimension 1,20\n",
    "        # Final linear layer\n",
    "        return self.lin(self.pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is designed to ouput two values: mean and standard deviation of the halo mass poserior. Therefore the follwoing loss function is minimized:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L = \\log{(\\sum_{i \\in batch} (y_{truth,i}-y_{infer,i})^2}) + \\log{(\\sum_{i \\in batch} ((y_{truth,i}-y_{infer,i})^2-\\sigma_i^2)^2}) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the training routine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU usage would decrease the execution time, therefore if available it should be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation and translation invariance are already statisfied by message passing and graph design. Rotating all subhalos around the center of the halo should also leave the global halo properties unchanged. To ensure rotational invariance we randomly perform rotations on each graph at every training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as Rot\n",
    "\n",
    "def train(loader, model, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    loss_tot = 0\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "\n",
    "        # Rotate randomly for data augmentation\n",
    "        rotmat = Rot.random().as_matrix()\n",
    "        data.pos = torch.tensor([rotmat.dot(p) for p in data.pos], dtype=torch.float32)\n",
    "        data.x[:,:3] = torch.tensor([rotmat.dot(p) for p in data.x[:,:3]], dtype=torch.float32)\n",
    "\n",
    "        data.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        y_out, err_out = out[:,0], out[:,1]     # Take mean and standard deviation of the output\n",
    "\n",
    "        # Compute loss as sum of two terms for likelihood-free inference\n",
    "        loss_mse = torch.mean((y_out - data.y)**2 , axis=0)\n",
    "        loss_lfi = torch.mean(((y_out - data.y)**2 - err_out**2)**2, axis=0)\n",
    "        loss = torch.log(loss_mse) + torch.log(loss_lfi)\n",
    "\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        loss_tot += loss.item()\n",
    "\n",
    "    return loss_tot/len(loader) #mean training loss per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test(loader, model, params):\n",
    "    model.eval()\n",
    "\n",
    "    errs = []\n",
    "    loss_tot = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        with torch.no_grad():\n",
    "\n",
    "            data.to(device)\n",
    "            out = model(data)  # Perform a single forward pass.\n",
    "            y_out, err_out = out[:,0], out[:,1] #mean and std\n",
    "            err = (y_out.reshape(-1) - data.y)/data.y #relative error of each sample of one batch\n",
    "            errs.append( np.abs(err.detach().cpu().numpy()).mean(axis=0) ) #mean relative error of one batch\n",
    "\n",
    "            # Compute loss as sum of two terms for likelihood-free inference\n",
    "            loss_mse = torch.mean((y_out - data.y)**2 , axis=0)\n",
    "            loss_lfi = torch.mean(((y_out - data.y)**2 - err_out**2)**2, axis=0)\n",
    "            loss = torch.log(loss_mse) + torch.log(loss_lfi)\n",
    "            loss_tot += loss.item() #calculate total loss of all batches\n",
    "\n",
    "\n",
    "    return loss_tot/len(loader), np.array(errs).mean(axis=0) #output mean loss and std loss of target per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full training routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_routine(model, train_loader, test_loader, params, verbose=True):\n",
    "\n",
    "    use_model, learning_rate, weight_decay, n_layers, k_nn, n_epochs, training, simsuite, simset, n_sims = params\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) #choosing the ADAM optimizer for the weights\n",
    "\n",
    "    train_losses, valid_losses = [], []\n",
    "    valid_loss_min, err_min = 1000., 1000.\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = train(train_loader, model, optimizer)\n",
    "        test_loss, err = test(test_loader, model, params)\n",
    "        train_losses.append(train_loss); valid_losses.append(test_loss) \n",
    "\n",
    "        # Save model if it has improved\n",
    "        if test_loss <= valid_loss_min: #if current test loss is smaller than the previous test_loss save model\n",
    "            if verbose: print(\"Validation loss decreased ({:.2e} --> {:.2e}).  Saving model ...\".format(valid_loss_min,test_loss))\n",
    "            torch.save(model.state_dict(), \"Models/\"+namemodel(params))\n",
    "            valid_loss_min = test_loss #current test loss is now the new value to beat\n",
    "            err_min = err \n",
    "\n",
    "        if verbose: print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.2e}, Validation Loss: {test_loss:.2e}, Relative error: {err:.2e}')\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
