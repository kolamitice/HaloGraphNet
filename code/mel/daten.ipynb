{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data used to train the neural network was gathered from the Cosmology and Astrophysics with MachinE Learning Simulations (CAMELS). CAMELS contains 6325 cosmological organized in different suits:     \n",
    "\n",
    "- magnetohydrodynamic:\n",
    "    - IllustrisTNG\n",
    "    - SIMBA\n",
    "    - Astrid\n",
    "- N-body\n",
    "\n",
    "Each suit is split into 4 different simulation sets:\n",
    "\n",
    "- LH (Latin-Hypercube): 1000 simulations with different values of the cosmological and astrophysical parameters which are arranged in a latin-hypercube. The initial conditions of each simulation are also different.\n",
    "- 1P (1-parameter at a time): 61 simulations where the values of the cosmological and astrophysical parameters are varied one at a time. The initial conditions of each simulation are the same.\n",
    "- CV (Cosmic Variance): 27 simulations where the cosomological and astrophysical parameters are fixed but the initial conditions vary.\n",
    "- EX (Extreme): 4 simulations with fixed cosmological parameters but different astrophysical parameters\n",
    "\n",
    "CAMELS vary two cosmological and 4 astrophysical parameters:\n",
    "\n",
    "- cosmological:\n",
    "    - $\\Omega_m$: Description. range of variation: $0.1 \\leq \\Omega_m \\leq 0.5$\n",
    "    - $\\sigma_8$: Description. range of variation: $0.6 \\leq \\sigma_8 \\leq 1.0$\n",
    "\n",
    "- astrophysical:\n",
    "    - $A_{SN1}$: Description. range of variation: $0.25 \\leq A_{SN1} \\leq 4.0$\n",
    "    - $A_{SN2}$: Description. range of variation: $0.50 \\leq A_{SN2} \\leq 2.0$\n",
    "    - $A_{GN1}$: Description. range of variation: $0.25 \\leq A_{GN1} \\leq 4.0$\n",
    "    - $A_{GN2}$: Description. range of variation: $0.50 \\leq A_{GN2} \\leq 2.0$\n",
    "\n",
    "Each hydrodynamic simulation evolves 256³ dark matter particles and 256³ gas resolution elements within a periodic comoving volume of 25 h⁻¹Mpc³ from a redshift z= 127 to z=0. During the evolution snapshots of the universe as a function of the redshift are created. In this work just data from the final snapshot, which corresponds to our universe at its current state, were used. We also restrict the data for the purpose of reasonable computation duration to IllustrisTNG-CV simulations. Details zu fof subfindhalos??\n",
    "Further details on the project and its goals are explained in [quelle: simulation_data.pdf][website]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes as h5py files containing a tremendous amount of information(quelle: https://www.tng-project.org/data/docs/specifications/#sec2\n",
    ") about the halos and subhalos (galaxies) in the universe. Since we are intereseted in infering the mass of a halo from a few distinct properties of its subhalos we must extract the desired features:\n",
    "- halo features:\n",
    "    - Halo mass\n",
    "    - normalized 3D position\n",
    "    - normalized 3D velocity\n",
    "- subhalo features\n",
    "    - Mass of star & wind particles ?? whyyy\n",
    "    - normalized 3D position\n",
    "    - normalized 3D velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from constants import *\n",
    "\n",
    "#--- FEATURES CHOICES ---#\n",
    "\n",
    "use_hmR = 1     # 1 for using the half-mass radius as feature\n",
    "use_vel = 1     # 1 for using subhalo velocity as feature\n",
    "only_positions = 0  # 1 for using only positions as features\n",
    "galcen_frame = 0    # 1 for writing positions and velocities in the central galaxy rest frame (otherwise it uses the total center of mass)\n",
    "\n",
    "#--- NORMALIZATION ---#\n",
    "\n",
    "Nstar_th = 10   # Minimum number of stellar particles required to consider a galaxy\n",
    "radnorm = 8.    # Ad hoc normalization for half-mass radius\n",
    "velnorm = 100.  # Ad hoc normalization for velocity. Use velnorm=1. for galcen_frame=1\n",
    "\n",
    "def general_tab(path):\n",
    "\n",
    "    # Read hdf5 file\n",
    "    f = h5py.File(path, 'r')\n",
    "\n",
    "    # Load subhalo features\n",
    "    #types = (Gas, Dark Matter, unused, Tracers, Stars & Wind particles, Black holes)Quelle: https://www.tng-project.org/data/docs/specifications/#sec2\n",
    "    SubhaloPos = f[\"Subhalo/SubhaloPos\"][:]/boxsize\n",
    "    SubhaloMassType = f[\"Subhalo/SubhaloMassType\"][:,4]\n",
    "    SubhaloVel = f[\"Subhalo/SubhaloVel\"][:]/velnorm\n",
    "\n",
    "    # Load halo features\n",
    "    HaloMass = f[\"Group/Group_M_Crit200\"][:]\n",
    "    GroupPos = f[\"Group/GroupPos\"][:]/boxsize\n",
    "    GroupVel = f[\"Group/GroupVel\"][:]/velnorm\n",
    "\n",
    "    # restriction features\n",
    "    HaloID = np.array(f[\"Subhalo/SubhaloGrNr\"][:], dtype=np.int32)\n",
    "    SubhaloLenType = f[\"Subhalo/SubhaloLenType\"][:,4]\n",
    "    SubhaloHalfmassRadType = f[\"Subhalo/SubhaloHalfmassRadType\"][:,4]/radnorm\n",
    "    \n",
    "    # Create general table with subhalo properties\n",
    "    # Host halo ID, 3D position, stellar mass, number of stellar particles, stellar half-mass radius, 3D velocity\n",
    "    tab = np.column_stack((HaloID, SubhaloPos, SubhaloMassType, SubhaloLenType, SubhaloHalfmassRadType, SubhaloVel))\n",
    "\n",
    "    # Restrictions:\n",
    "    indexes = np.argwhere(HaloMass>0.).reshape(-1)  # Neglect halos with zero mass\n",
    "    tab = tab[tab[:,4]>0.]                          # restrict to subhalos with mass (stars)\n",
    "    tab = tab[tab[:,5]>Nstar_th]                    # restrict to subhalos with a minimum of star/wind particles\n",
    "    tab[:,4] = np.log10(tab[:,4])                   # take the log of the stellar mass\n",
    "\n",
    "    # Once restricted to a minimum number of stellar particles, remove this feature since it is not observable\n",
    "    tab = np.delete(tab, 5, 1)\n",
    "\n",
    "    if not use_hmR:\n",
    "        tab = np.delete(tab, 5, 1)  # remove SubhaloHalfmassRadType if not required\n",
    "\n",
    "    if only_positions:\n",
    "        tab = np.column_stack((tab[:,0],tab[:,1],tab[:,2],tab[:,3]))\n",
    "\n",
    "    f.close()\n",
    "    return tab, HaloMass, GroupPos, GroupVel, indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??Boundary conditions??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct periodic boundary effects\n",
    "# Some halos close to a boundary could have subhalos at the other extreme of the box, due to periodic boundary conditions\n",
    "# Just add or substract a length boxe in such cases to correct this artifact\n",
    "def correct_boundary(pos, boxlength=1.):\n",
    "\n",
    "    for i, pos_i in enumerate(pos):\n",
    "        for j, coord in enumerate(pos_i):\n",
    "            if coord > boxlength/2.:\n",
    "                pos[i,j] -= boxlength\n",
    "            elif -coord > boxlength/2.:\n",
    "                pos[i,j] += boxlength\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the neural net one has to split the data into training, test and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, DataLoader\n",
    "import random\n",
    "\n",
    "def split_datasets(dataset):\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    num_train = len(dataset)\n",
    "    split_valid = int(np.floor(valid_size * num_train))\n",
    "    split_test = split_valid + int(np.floor(test_size * num_train))\n",
    "\n",
    "    train_dataset = dataset[split_test:]\n",
    "    valid_dataset = dataset[:split_valid]\n",
    "    test_dataset = dataset[split_valid:split_test]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, DataLoader\n",
    "from Source.constants import *\n",
    "from Source.plotting import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
